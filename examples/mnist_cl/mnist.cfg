############ General parameters ############
task=train
prediction_type=classif
# Use input_model for finetuning a pre-trained model
#input_model=
output_model=model.dat
out_pred=./pred.txt
eval_test=1
eval_period=2000
save_model=100000
nb_pred=4096
max_batches=800000
# Available data_format mnist/list/csv/bin
data_format=mnist

############ Data parameters ############
source_train = ./train-images.idx3-ubyte
label_train = train-labels.idx1-ubyte
# if data is in range [0;1], set is_norm=1 else set is_norm=0
train_is_norm=0
# Online data augmentation
do_rand_translate=1
range_shift_x=5
range_shift_y=5
do_rand_scale=0
min_scale=0.85
max_scale=1.15
do_rand_rotate=1
rotation_range=30
# rotation_range=theta for [-theta/2;theta/2] (theta in degree)
source_test=t10k-images.idx3-ubyte
label_test = t10k-labels.idx1-ubyte
test_is_norm=0
input_width=28
input_height=28
input_channels=1
batch_size=16

############ Solver parameters ############
momentum=0.9
decay=0.0005
learning_rate=0.003
policy=sigmoid
gamma=.00002
step=400000

############ Network architecture ############
{convolutional}
filters=16
size=3
stride=1
pad=1
init=xavier
src=input
dst=conv1

{activation}
function=relu
src=conv1

{maxpool}
size=2
stride=2
src=conv1
dst=pool1

{convolutional}
filters=16
size=3
stride=1
pad=1
init=xavier
src=pool1
dst=conv2

{activation}
function=relu
src=conv2

{maxpool}
size=2
stride=2
src=conv2
dst=pool2

{connected}
output=256
init=xavier
src=pool2
dst=fc1

{activation}
function=relu
src=fc1

{connected}
output=10
init=xavier
src=fc1
dst=fc2

{activation}
function=relu
src=fc2

{softmax}
src=fc2
dst=soft

{cost}
src=soft
dst=out
metric=error

